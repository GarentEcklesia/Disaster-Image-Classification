{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df652230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba0fe3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "version = tf.__version__\n",
    "print(\"TensorFlow Version:\", version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dirs = {\n",
    "    'Earthquake': 'Comprehensive Disaster Dataset(CDD)/Damaged_Infrastructure/Earthquake',\n",
    "    'Urban_Fire': 'Comprehensive Disaster Dataset(CDD)/Fire_Disaster/Urban_Fire',\n",
    "    'Land_Slide': 'Comprehensive Disaster Dataset(CDD)/Land_Disaster/Land_Slide',\n",
    "    'Water_Disaster': 'Comprehensive Disaster Dataset(CDD)/Water_Disaster'\n",
    "}\n",
    "\n",
    "target_root = 'dataset'\n",
    "\n",
    "os.makedirs(target_root, exist_ok=True)\n",
    "\n",
    "for class_name, src_dir in src_dirs.items():\n",
    "    target_dir = os.path.join(target_root, class_name)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for fname in os.listdir(src_dir):\n",
    "        src_path = os.path.join(src_dir, fname)\n",
    "        dst_path = os.path.join(target_dir, fname)\n",
    "        # Only copy image files\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not copy {src_path}: {e}\")\n",
    "\n",
    "print(\"Dataset reorganized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbbd925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1942 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    target_root,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical', \n",
    "    image_size=(224, 224),     \n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd986cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earthquake', 'Land_Slide', 'Urban_Fire', 'Water_Disaster']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e9f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (32, 224, 224, 3)\n",
      "Label batch shape: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataset.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319b0c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9iUlEQVR4nO3dB3hTZR/38T9771VQlsADFNkoMkRlDweCioqAWsHB3lRkIyAKKMgQHgQHCgqCCMqQIQplg0wZylKWCrTMMpr3+t/ve/ImbYGWJ23S3N/PdYWSc06SOznJyS/3OqlcLpdLAAAALJba3wUAAADwNwIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhGAOIoVKyYvvviipHSDBw+WVKlSJctjPfzww+biWL16tXnsuXPnJsvj6/7S/QbgzhCIAIv8/vvv8uqrr8o999wjGTNmlOzZs0utWrXkgw8+kMuXL0sgmzlzpgkYzkXLX6hQIWnUqJGMHz9ezp8/75PHOX78uAlS27dvl0ATyGUDUrq0/i4AgOSxePFiefrppyVDhgzStm1buffee+Xq1avyyy+/SO/evWX37t0ydepUCXRDhw6V4sWLy7Vr1+TkyZOmJqZbt24yduxYWbhwoVSoUMG97VtvvSX9+vVLdOgYMmSIqW2pVKlSgm+3bNkySWq3Ktu0adMkJiYmycsABCsCEWCBQ4cOybPPPitFixaVlStXSsGCBd3rOnbsKAcPHjSBKSVo0qSJVKtWzX09PDzcPKdHH31UHn/8cdm7d69kypTJrEubNq25JKVLly5J5syZJX369OJP6dKl8+vjAykdTWaABUaPHi0XLlyQ6dOne4UhR8mSJaVr1643vf2ZM2ekV69eUr58ecmaNatpatNg8uuvv8bZdsKECVKuXDkTEnLlymXCyxdffOFer01bWqOjtRxaW5U/f35p0KCBbN269Y6fX926dWXAgAFy5MgR+fzzz2/Zh2j58uVSu3ZtyZkzp3kupUuXljfffNOs09qm++67z/z/pZdecjfPaXOd0j5CWrO2ZcsWqVOnjnmOzm1j9yFy3Lhxw2wTEhIiWbJkMaHt2LFjCeqz5XmftytbfH2ILl68KD179pTChQub11qf63vvvScul8trO72fTp06yYIFC8zz0211Hy5ZsiQRewFI2aghAizw3XffmX5DNWvWvKPb//HHH+bLUpvctLnq1KlT8tFHH8lDDz0ke/bsMX15nGabLl26yFNPPWUC1pUrV2THjh2yYcMGef755802r732mulorF/AoaGh8u+//5pmO63ZqVKlyh0/xzZt2pjgoU1X7du3j3cbbRbUmiRtVtOmN/3i19qxtWvXmvVly5Y1ywcOHCgdOnSQBx980Cz3fN20vBoGtcbthRdekAIFCtyyXG+//bYJHH379pXTp0/L+++/L/Xr1zf9gJyarIRISNk8aejR8LVq1SoJCwszTWxLly41zaN//fWXjBs3zmt73QfffPONvPHGG5ItWzbTL6tly5Zy9OhRyZMnT4LLCaRYLgBBLTIyUqsDXE888USCb1O0aFFXu3bt3NevXLniunHjhtc2hw4dcmXIkME1dOhQ9zJ9jHLlyt3yvnPkyOHq2LGjK7FmzJhhnsemTZtued+VK1d2Xx80aJC5jWPcuHHm+t9//33T+9D712308WJ76KGHzLopU6bEu04vjlWrVplt77rrLldUVJR7+VdffWWWf/DBBzd9vW92n7cqm95e78exYMECs+3w4cO9tnvqqadcqVKlch08eNC9TLdLnz6917Jff/3VLJ8wYcJNXikguNBkBgS5qKgo81d/9d8prUlJnTq1uwlIa0mc5ibPpi5thvrzzz9l06ZNN70v3UZrjLSDsK9pmW412kwfW3377bd33AFZXwttskoo7cDu+dpr7Zk2W37//feSlPT+06RJY2rsPGkTmmagH374wWu51lqVKFHCfV1r0bRpVGsHARsQiIAgp19q6n8Zlq7hQZtYSpUqZQJB3rx5JV++fKY5LDIy0r2dNgtpKLn//vvNttph22mO8uzPtGvXLtOvRbfTfj6++tLVflK3Cn6tWrUy0wy88sorpqlLm72++uqrRIWju+66K1EdqPV18KTNZ9pn6/Dhw5KUtD+VNmXGfj206c1Z76lIkSJx7kP7gJ09ezZJywkECgIRYEEg0i9GDSF3asSIEdKjRw/TkVg7LWtfFO2crB1vPcOEftnu27dPZs+ebTouz5s3z/wdNGiQe5tnnnnGBCDtfK3levfdd839xK6xSCytmdJwpmHjZrTPzpo1a+THH380fY400GlI0k7dWvOVEInp95NQN5s8MqFl8gWtTYpP7A7YQLAiEAEW0I7EOiljRETEHd1eO0E/8sgjZpSa1qo0bNjQNLGcO3cuzrY6kkpDxowZM0yH3GbNmpmOxdrB2qFNRtp5Vztq65QA2mlXt/lffPbZZ+avTtR4K9r0V69ePTNvkXYI18fVYfva+Vj5embrAwcOxAkY2pHbc0SY1sTE91rGrsVJTNl0igVtloxdM/jbb7+51wP4/whEgAX69Oljgoo2FekIsdg0LOls1beqPYhdU/D111+b0UqetG+RJ21a0pFkeludSFFrPDyb2JQOu9eaoujo6Dt8dmICzbBhw8wIuNatW99y+oDYnAkOncfX10nFF1DuxKeffuoVSjRcnjhxwoxUc2jfnfXr15uJMh2LFi2KMzw/MWVr2rSpeb0//PBDr+Xa9KnByvPxATDsHrCCfuHqXEBac6PNWp4zVa9bt86Em1udu0xrmHTIt3Ym1mHeO3fulFmzZpmh/J605kjn29F+OtpHR4fS6xey1hJpXxb9Ir/77rtNx+KKFSua/kbafKWdsMeMGZOg56JNa1rLcf36dRPuNAxp853WeOhM1XpKj5vR56BNZloe3V6HwU+aNMmUSZv2nNdKO19PmTLFlFlDSPXq1U3YuhO5c+c2962vnZZXh91rs57n1AAaVDUoNW7c2DQpakDVpknPTs6JLdtjjz1mavX69+9v+ivp661TEmiHcp0HKvZ9A9bz9zA3AMln//79rvbt27uKFStmhllny5bNVatWLTO0WofW32rYfc+ePV0FCxZ0ZcqUydwmIiIizrDwjz76yFWnTh1Xnjx5zJD8EiVKuHr37m2G/qvo6GhzvWLFiuaxs2TJYv4/adKkBA+7dy5a/pCQEFeDBg3MEHbPoe03G3a/YsUKMzVAoUKFzO3173PPPWdeF0/ffvutKzQ01JU2bVqvYe76XG82rcDNht1/+eWXrvDwcFf+/PnNa9esWTPXkSNH4tx+zJgxZoi+vm76+m7evDnOfd6qbLGH3avz58+7unfvbp5nunTpXKVKlXK9++67rpiYGK/t9H7imwrhZtMBAMEolf7j71AGAADgT/QhAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHhMzJoCeq0mnwNeJ0Hw9rT8AAEgaOrOQzhSvs+HraXtuhUCUABqG9MzcAAAg5dHT4OiM9LdCIEoArRlyXlA9czgAAAh8UVFRpkLD+R6/FQJRAjjNZBqGCEQAAKQsCenuQqdqAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPXS+rsAAAAEimL9Fvu7CNY6PKqZXx+fGiIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOv5NRCtWbNGHnvsMSlUqJCkSpVKFixY4LXe5XLJwIEDpWDBgpIpUyapX7++HDhwwGubM2fOSOvWrSV79uySM2dOCQsLkwsXLnhts2PHDnnwwQclY8aMUrhwYRk9enSyPD8AAJAy+DUQXbx4USpWrCgTJ06Md70Gl/Hjx8uUKVNkw4YNkiVLFmnUqJFcuXLFvY2God27d8vy5ctl0aJFJmR16NDBvT4qKkoaNmwoRYsWlS1btsi7774rgwcPlqlTpybLcwQAAIEvlUurYQKA1hDNnz9fmjdvbq5rsbTmqGfPntKrVy+zLDIyUgoUKCAzZ86UZ599Vvbu3SuhoaGyadMmqVatmtlmyZIl0rRpU/nzzz/N7SdPniz9+/eXkydPSvr06c02/fr1M7VRv/32W4LKpqEqR44c5vG1JgoAEJyYmDG4JmZMzPd3wPYhOnTokAkx2kzm0CdVvXp1iYiIMNf1rzaTOWFI6fapU6c2NUrONnXq1HGHIaW1TPv27ZOzZ8/G+9jR0dHmRfS8AACA4BWwgUjDkNIaIU963Vmnf/Pnz++1Pm3atJI7d26vbeK7D8/HiG3kyJEmfDkX7XcEAACCV8AGIn8KDw831WvO5dixY/4uEgAAsDEQhYSEmL+nTp3yWq7XnXX69/Tp017rr1+/bkaeeW4T3314PkZsGTJkMG2NnhcAABC8AjYQFS9e3ASWFStWuJdpXx7tG1SjRg1zXf+eO3fOjB5zrFy5UmJiYkxfI2cbHXl27do19zY6Iq106dKSK1euZH1OAAAgMPk1EOl8Qdu3bzcXpyO1/v/o0aNm1Fm3bt1k+PDhsnDhQtm5c6e0bdvWjBxzRqKVLVtWGjduLO3bt5eNGzfK2rVrpVOnTmYEmm6nnn/+edOhWucn0uH5c+bMkQ8++EB69Ojhz6cOAAACSFp/PvjmzZvlkUcecV93Qkq7du3M0Po+ffqYuYp0XiGtCapdu7YZVq8TLDpmzZplQlC9evXM6LKWLVuauYsc2il62bJl0rFjR6latarkzZvXTPboOVcRAACwW8DMQxTImIcIAOzAPET+wzxEAAAAfkYgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsF9CB6MaNGzJgwAApXry4ZMqUSUqUKCHDhg0Tl8vl3kb/P3DgQClYsKDZpn79+nLgwAGv+zlz5oy0bt1asmfPLjlz5pSwsDC5cOGCH54RAAAIRAEdiN555x2ZPHmyfPjhh7J3715zffTo0TJhwgT3Nnp9/PjxMmXKFNmwYYNkyZJFGjVqJFeuXHFvo2Fo9+7dsnz5clm0aJGsWbNGOnTo4KdnBQAAAk0ql2d1S4B59NFHpUCBAjJ9+nT3spYtW5qaoM8//9zUDhUqVEh69uwpvXr1MusjIyPNbWbOnCnPPvusCVKhoaGyadMmqVatmtlmyZIl0rRpU/nzzz/N7W8nKipKcuTIYe5ba5kAAMGpWL/F/i6CtQ6Paubz+0zM93dA1xDVrFlTVqxYIfv37zfXf/31V/nll1+kSZMm5vqhQ4fk5MmTppnMoU+8evXqEhERYa7rX20mc8KQ0u1Tp05tapQAAADSSgDr16+fSXdlypSRNGnSmD5Fb7/9tmkCUxqGlNYIedLrzjr9mz9/fq/1adOmldy5c7u3iS06OtpcHFoGAAAQvAK6huirr76SWbNmyRdffCFbt26VTz75RN577z3zNymNHDnS1DQ5l8KFCyfp4wEAAP8K6EDUu3dvU0ukfYHKly8vbdq0ke7du5vAokJCQszfU6dOed1Orzvr9O/p06e91l+/ft2MPHO2iS08PNy0NzqXY8eOJdEzBAAAgSCgA9GlS5dMXx9P2nQWExNj/q/D8TXUaD8jz+Yt7RtUo0YNc13/njt3TrZs2eLeZuXKleY+tK9RfDJkyGA6X3leAABA8AroPkSPPfaY6TNUpEgRKVeunGzbtk3Gjh0rL7/8slmfKlUq6datmwwfPlxKlSplApLOW6Qjx5o3b262KVu2rDRu3Fjat29vhuZfu3ZNOnXqZGqdEjLCDAAABL+ADkQ635AGnDfeeMM0e2mAefXVV81EjI4+ffrIxYsXzbxCWhNUu3ZtM6w+Y8aM7m20H5KGoHr16pkaJx26r3MXAQAABPw8RIGCeYgAwA7MQ+Q/zEMEAADgZwQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYL1EB6JPPvlEFi9e7L7ep08fyZkzp9SsWVOOHDni6/IBAAAEXiAaMWKEZMqUyfw/IiJCJk6cKKNHj5a8efNK9+7dk6KMAAAASSptYm9w7NgxKVmypPn/ggULpGXLltKhQwepVauWPPzww0lRRgAAgMCqIcqaNav8+++/5v/Lli2TBg0amP9nzJhRLl++7PsSAgAABFoNkQagV155RSpXriz79++Xpk2bmuW7d++WYsWKJUUZAQAAAquGSPsM1ahRQ/7++2+ZN2+e5MmTxyzfsmWLPPfcc0lRRgAAgMCqIdIRZR9++GGc5UOGDPFVmQAAAAJ/HqKff/5ZXnjhBTPU/q+//jLLPvvsM/nll198XT4AAIDAC0TaTNaoUSMz9H7r1q0SHR1tlkdGRpoh+QAAAEEfiIYPHy5TpkyRadOmSbp06dzLddi9BiQAAICgD0T79u2TOnXqxFmeI0cOOXfunK/KBQAAELiBKCQkRA4ePBhnufYfuueee3xVLgAAgMANRO3bt5euXbvKhg0bJFWqVHL8+HGZNWuW9OrVS15//fWkKSUAAEAgDbvv16+fxMTESL169eTSpUum+SxDhgwmEHXu3DlpSgkAABBIgUhrhfr37y+9e/c2TWcXLlyQ0NBQc0oPAAAAKwKRI3369CYIAQAAWBeInnzySVNLFJsu0xO8lixZUp5//nkpXbq0r8oIAAAQWJ2qdXj9ypUrzZxDGoL0sm3bNrPs+vXrMmfOHKlYsaKsXbs2aUoMAADg7xoiHXavNUB6PrPUqf9vntJO1jryLFu2bDJ79mx57bXXpG/fvpzKAwAABGcN0fTp06Vbt27uMGTuJHVqM8Js6tSppsaoU6dOsmvXLl+XFQAAIDACkTaL/fbbb3GW67IbN26Y/2tfovj6GQEAAARFk1mbNm0kLCxM3nzzTbnvvvvMsk2bNpkTu7Zt29Zc/+mnn6RcuXK+Ly0AAEAgBKJx48ZJgQIFZPTo0XLq1CmzTK93797d9BtSDRs2lMaNG/u+tAAAAEkglcvlct3pjaOioszf7NmzSzDT56mj6yIjI4P+uQKAzYr1W+zvIljr8Khmfv3+vuOJGRXhAAAABIM7CkRz586Vr776So4ePSpXr171WqfzEwEAAAT1KLPx48fLSy+9ZPoN6YSM999/v+TJk0f++OMPadKkSdKUEgAAIJAC0aRJk8x8QxMmTDDnM+vTp48sX75cunTpYtroAAAAgj4QaTNZzZo1zf8zZcok58+fdw/H//LLL31fQgAAgEALRHrqjjNnzpj/FylSRNavX2/+f+jQIfkfBqwBAACknEBUt25dWbhwofm/9iXS+YcaNGggrVq1kieffDIpyggAABBYo8y0/5CezFV17NjRdKhet26dPP744/Lqq68mRRkBAAACKxDpiVw9T+z67LPPmgsAAIBV8xBduXJFduzYIadPn3bXFjm0pggAACCoA9GSJUvMSVz/+eefOOv0DPfOGe8BAACCtlN1586d5emnn5YTJ06Y2iHPC2EIAABYEYj0DPc9evQwM1UDAABYGYieeuopWb16ddKUBgAAICX0Ifrwww9Nk9nPP/8s5cuXl3Tp0nmt11N4AAAABHUNkZ6eY9myZTJv3jxzPrNx48a5L++//77PC/jXX3/JCy+8YOY70lOFaAjbvHmze73Ojj1w4EApWLCgWV+/fn05cOCA133ozNqtW7eW7NmzS86cOSUsLEwuXLjg87ICAABLAlH//v1lyJAh5kSuhw8fNqfscC56xntfOnv2rNSqVcvUQv3www+yZ88eGTNmjOTKlcu9zejRo2X8+PEyZcoU2bBhg2TJkkUaNWpkpgZwaBjavXu3OQntokWLZM2aNdKhQweflhUAAKRcqVyJPAFZ7ty5ZdOmTVKiRAlJav369ZO1a9ea5rn4aNELFSokPXv2lF69epllGtS0w/fMmTPNhJF79+6V0NBQU+Zq1aq5pw5o2rSp/Pnnn+b2txMVFSU5cuQw9621TACA4FSs32J/F8Fah0c18/l9Jub7O9E1RO3atZM5c+ZIctBzpmmI0T5L+fPnl8qVK8u0adPc67VW6uTJk6aZzKFPvHr16hIREWGu619tJnPCkNLtdbZtrVECAABIdKdqnWtIm6mWLl0qFSpUiNOpeuzYsT4rnDbBTZ482Qzzf/PNN00tj3baTp8+vQlmGoZU7CkA9LqzTv9qmPKUNm1aU9PlbBNbdHS0uXgmTAAAELwSHYh27txpamrUrl274sxU7Us62aPW7IwYMcJc18fVx9T+QhqIksrIkSNNPykAAGCHRAeiVatWSXLRkWPa/8dT2bJlzQg3FRIS4p4sUrd16PVKlSq5t9Fzrnm6fv26GXnm3D628PBwUyvlWUNUuHBhHz4zAAAQSBLdhyg56Qizffv2eS3bv3+/FC1a1Py/ePHiJtSsWLHCK7xo36AaNWqY6/r33LlzsmXLFvc2K1euNLVP2tcoPhkyZDCdrzwvAAAgeCW4hqhFixYJ2u6bb74RX+nevbvUrFnTNJk988wzsnHjRpk6daq5OE103bp1k+HDh0upUqVMQBowYIAZOda8eXN3jVLjxo2lffv2pqnt2rVr0qlTJzMCLSEjzAAAQPBLcCDS0VvJ7b777pP58+ebJqyhQ4eawKOTP+q8Qo4+ffrIxYsXzbxCWhNUu3ZtM6w+Y8aM7m1mzZplQlC9evXM6LKWLVuauYsAAADuaB4iGzEPEQDYgXmI/CfFzUMEAAAQbAhEAADAegQiAABgPQIRAACwXoICUZUqVcyZ55WO9rp06VJSlwsAACCwApGeMV6Htis9pcWFCxeSulwAAACBNQ+RngbjpZdeMnP86Cj99957T7JmzRrvtgMHDvR1GQEAAPwfiGbOnCmDBg2SRYsWmdmhf/jhB3PG+Nh0HYEIAAAEZSAqXbq0zJ492/xfZ3rWc4flz58/qcsGAAAQmGe715OiAgAAWB2I1O+//27OKaadrVVoaKh07dpVSpQo4evyAQAABN48REuXLjUBSM88X6FCBXPZsGGDlCtXTpYvX540pQQAAAikGqJ+/fpJ9+7dZdSoUXGW9+3bVxo0aODL8gEAAAReDZE2k4WFhcVZ/vLLL8uePXt8VS4AAIDADUT58uWT7du3x1muyxh5BgAArGgya9++vXTo0EH++OMPqVmzplm2du1aeeedd6RHjx5JUUYAAIDACkQDBgyQbNmyyZgxYyQ8PNwsK1SokAwePFi6dOmSFGUEAAAIrECks1Frp2q9nD9/3izTgAQAAGDVPEQOghAAALCyUzUAAECwIRABAADrEYgAAID1EhWIrl27JvXq1ZMDBw4kXYkAAAACORClS5dOduzYkXSlAQAASAlNZi+88IJMnz49aUoDAACQEobdX79+XT7++GP58ccfpWrVqpIlSxav9WPHjvVl+QAAAAIvEO3atUuqVKli/r9///44kzYCAAAEfSBatWpV0pQEAAAgpQ27P3jwoCxdulQuX75srrtcLl+WCwAAIHAD0b///muG3v/nP/+Rpk2byokTJ8zysLAw6dmzZ1KUEQAAILACkZ7UVYffHz16VDJnzuxe3qpVK1myZImvywcAABB4fYiWLVtmmsruvvtur+WlSpWSI0eO+LJsAAAAgVlDdPHiRa+aIceZM2ckQ4YMvioXAABA4AaiBx98UD799FOvofYxMTEyevRoeeSRR3xdPgAAgMBrMtPgo52qN2/eLFevXpU+ffrI7t27TQ3R2rVrk6aUAAAAgVRDdO+995oJGWvXri1PPPGEaUJr0aKFbNu2TUqUKJE0pQQAAAikGiKVI0cO6d+/v+9LAwAAkFIC0dmzZ80JXvfu3Wuuh4aGyksvvSS5c+f2dfkAAAACr8lszZo1UqxYMRk/frwJRnrR/xcvXtysAwAACPoaoo4dO5pJGCdPnixp0qQxy27cuCFvvPGGWbdz586kKCcAAEDg1BDpOcz0FB1OGFL6/x49eph1AAAAQR+IqlSp4u475EmXVaxY0VflAgAACKwmsx07drj/36VLF+nataupDXrggQfMsvXr18vEiRNl1KhRSVdSAACAJJLK5XK5brdR6tSpzYzUt9tUt9H+RMEmKirKTDUQGRkp2bNn93dxAABJpFi/xf4ugrUOj2rm1+/vBNUQHTp0yFdlA6zCwTW4Dq4AgleCAlHRokWTviQAAAApaWLG48ePyy+//CKnT582J3b1pH2MAAAAgjoQzZw5U1599VVJnz695MmTx/Qbcuj/CUQAACDoA9GAAQNk4MCBEh4ebjpbAwAApHSJTjSXLl2SZ599ljAEAACCRqJTTVhYmHz99ddJUxoAAICU0GQ2cuRIefTRR2XJkiVSvnx5SZcundf6sWPH+rJ8AAAAgRmIli5dKqVLlzbXY3eqBgAACPpANGbMGPn444/lxRdfTJoSAQAABHofogwZMkitWrWSpjQAAAApoYZIT+w6YcIEGT9+fNKUCAACHKdk8R9OyYKACUQbN26UlStXyqJFi6RcuXJxOlV/8803viwfAABA4DWZ5cyZU1q0aCEPPfSQ5M2b15xF1vOSlEaNGmU6bnfr1s297MqVK9KxY0cza3bWrFmlZcuWcurUKa/bHT16VJo1ayaZM2eW/PnzS+/eveX69etJWlYAABDENUQzZswQf9i0aZN89NFHUqFCBa/l3bt3l8WLF5u5kTSQderUyQS2tWvXmvU3btwwYSgkJETWrVsnJ06ckLZt25qarREjRvjluQAAgMCSIqabvnDhgrRu3VqmTZsmuXLlci+PjIyU6dOnm7mP6tatK1WrVjWBTYPP+vXrzTbLli2TPXv2yOeffy6VKlWSJk2ayLBhw2TixIly9epVPz4rAACQYgNR8eLF5Z577rnpJSlok5jW8tSvX99r+ZYtW+TatWtey8uUKSNFihSRiIgIc13/6gSSBQoUcG/TqFEjiYqKkt27dydJeQEAQJA3mXn231EaSLZt22Zmrta+Ob42e/Zs2bp1q2kyi+3kyZOSPn1606/Jk4YfXeds4xmGnPXOuvhER0ebi0PDEwAACF53NOw+PtoEtXnzZvGlY8eOmcdbvny5ZMyYUZKLzsY9ZMiQZHs8AAAQJH2ItG/OvHnzxJe0Sez06dNSpUoVSZs2rbn89NNPZg4k/b/W9Gg/oHPnznndTkeZaSdqpX9jjzpzrjvbxBYeHm76JzkXDWYAACB4+SwQzZ07V3Lnzi2+VK9ePdm5c6ds377dfalWrZrpYO38X0eLrVixwn2bffv2mWH2NWrUMNf1r96HBiuH1jhlz55dQkNDbzobt673vAAAgOCV6CazypUre53E1eVymb44f//9t0yaNMmnhcuWLZvce++9XsuyZMli5hxyloeFhUmPHj1MGNPg0rlzZxOCHnjgAbO+YcOGJvi0adNGRo8ebcr61ltvmY7aGnwAAAASHYiaN2/udT116tSSL18+efjhh80Ir+Q2btw4UwadkFE7QusIMs9gliZNGjOr9uuvv26Ckgaqdu3aydChQ5O9rAAAIEgC0aBBg8SfVq9e7XVdO1trh2693EzRokXl+++/T4bSAQCAlChFTMwIAAAQEDVE2izl2XcoPrqec4QBAICgDUTz58+/6TqdDVqHwsfExPiqXAAAAIEXiJ544ok4y3SIe79+/eS7774zQ+HpqAwAAKzpQ3T8+HFp3769OUeYNpHpnECffPKJ6bwMAAAQ1IFIZ23u27evlCxZ0pwYVSdE1Nqh2HMFAQAABGWTmU5q+M4775jTXXz55ZfxNqEBAAAEdSDSvkKZMmUytUPaPKaX+HzzzTe+LB8AAEDgBKK2bdvedtg9AABAUAeimTNnJm1JAAAA/ISZqgEAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYL6EA0cuRIue+++yRbtmySP39+ad68uezbt89rmytXrkjHjh0lT548kjVrVmnZsqWcOnXKa5ujR49Ks2bNJHPmzOZ+evfuLdevX0/mZwMAAAJVQAein376yYSd9evXy/Lly+XatWvSsGFDuXjxonub7t27y3fffSdff/212f748ePSokUL9/obN26YMHT16lVZt26dfPLJJzJz5kwZOHCgn54VAAAINGklgC1ZssTrugYZreHZsmWL1KlTRyIjI2X69OnyxRdfSN26dc02M2bMkLJly5oQ9cADD8iyZctkz5498uOPP0qBAgWkUqVKMmzYMOnbt68MHjxY0qdP76dnBwAAAkVA1xDFpgFI5c6d2/zVYKS1RvXr13dvU6ZMGSlSpIhERESY6/q3fPnyJgw5GjVqJFFRUbJ79+5kfw4AACDwBHQNkaeYmBjp1q2b1KpVS+69916z7OTJk6aGJ2fOnF7bavjRdc42nmHIWe+si090dLS5ODQ8AQCA4JViaoi0L9GuXbtk9uzZydKZO0eOHO5L4cKFk/wxAQCA/6SIQNSpUydZtGiRrFq1Su6++2738pCQENNZ+ty5c17b6ygzXedsE3vUmXPd2Sa28PBw0zznXI4dO5YEzwoAAASKgA5ELpfLhKH58+fLypUrpXjx4l7rq1atKunSpZMVK1a4l+mwfB1mX6NGDXNd/+7cuVNOnz7t3kZHrGXPnl1CQ0PjfdwMGTKY9Z4XAAAQvNIGejOZjiD79ttvzVxETp8fbcbKlCmT+RsWFiY9evQwHa01uHTu3NmEIB1hpnSYvgafNm3ayOjRo819vPXWW+a+NfgAAAAEdCCaPHmy+fvwww97Ldeh9S+++KL5/7hx4yR16tRmQkbtCK0jyCZNmuTeNk2aNKa57fXXXzdBKUuWLNKuXTsZOnRoMj8bAAAQqNIGepPZ7WTMmFEmTpxoLjdTtGhR+f77731cOgAAECwCug8RAABAciAQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1iMQAQAA6xGIAACA9QhEAADAegQiAABgPQIRAACwHoEIAABYj0AEAACsRyACAADWIxABAADrEYgAAID1CEQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAArEcgAgAA1kvr7wJApFi/xf4ugrUOj2rm7yIAAAIANUQAAMB6BCIAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOtZFYgmTpwoxYoVk4wZM0r16tVl48aN/i4SAAAIANYEojlz5kiPHj1k0KBBsnXrVqlYsaI0atRITp8+7e+iAQAAP7MmEI0dO1bat28vL730koSGhsqUKVMkc+bM8vHHH/u7aAAAwM+sCERXr16VLVu2SP369d3LUqdOba5HRET4tWwAAMD/rDiX2T///CM3btyQAgUKeC3X67/99luc7aOjo83FERkZaf5GRUUlSflioi8lyf3i9pJqnzrYt8G5b9mv/sNnNnhFJcG+de7T5XLddlsrAlFijRw5UoYMGRJneeHChf1SHiSdHO/7uwRIKuzb4MR+DV45knDfnj9/XnLkyHHLbawIRHnz5pU0adLIqVOnvJbr9ZCQkDjbh4eHmw7YjpiYGDlz5ozkyZNHUqVKlSxlTgk0eWtIPHbsmGTPnt3fxYEPsW+DF/s2OLFf46c1QxqGChUqJLdjRSBKnz69VK1aVVasWCHNmzd3hxy93qlTpzjbZ8iQwVw85cyZM9nKm9Loh48PYHBi3wYv9m1wYr/GdbuaIasCkdIan3bt2km1atXk/vvvl/fff18uXrxoRp0BAAC7WROIWrVqJX///bcMHDhQTp48KZUqVZIlS5bE6WgNAADsY00gUto8Fl8TGe6MNivqRJexmxeR8rFvgxf7NjixX/93qVwJGYsGAAAQxKyYmBEAAOBWCEQAAMB6BCIAAGA9AhHiOHz4sJmAcvv27f4uSkCVJRi9+OKL7rm5fEH31YIFCxK871avXm22OXfunM/KEGyKFStmpgmx5T2EpPHwww9Lt27d/F2MgEYgSiH0oKNfHLEvjRs3/p/vl4NZYAj0faHTVrz++utSpEgRM5JFZ3lv1KiRrF27Nt7tddbcEydOyL333iu2u9mX0cyZM1PEpK+DBw+O9/jz448/ygcffGCeR0o0ZcoUyZYtm1y/ft297MKFC5IuXTqzz+IL77///vst7zM5Q76+7s6+0LMx5MqVS6pXry5Dhw51n4PT8c0338iwYcMkOaTy+GGUklg17D6l0/AzY8YMr2V3OsRST3bLaUiQGC1btpSrV6/KJ598Ivfcc4859Y3O9v7vv//Gu70eoOM7NQ4SRl9rnWU/UJQrV84EIE+5c+e+bRkD7Xl4euSRR0wA2rx5szzwwANm2c8//2zetxs2bJArV65IxowZzfJVq1aZHwMlSpRIlrLpAHA9TqdNe+uvaZ2Vet++fWZ7DWHr1q0z5+PU7wr9seKcskL3VUpz7do1E06TCzVEKYjzq9zzor8I1NixY6V8+fKSJUsW88v8jTfeMB/02L9EFy5cKKGhoea+Xn75ZfPl9u2337p/ZeivG8cff/xhDhiZM2eWihUrSkREhFd59D71AKHrn3zySRkzZozXr934ajz0V7LnLy+dHLN27drmdnquuEcfffSWv8D0AKHlLlOmjBw9etQs0/JXqVLFHLj0i1pPzOv5iy8YJHT/Ll26VMqWLStZs2Y1AVpraDxfO52x3Xmt+/Tpk6AzQCs90OoXxTvvvGPeE0WLFjUzvut5/x5//PF4bxNfk9n3338v//nPfyRTpkzmfnSb2H755Rd58MEHzTb6XLt06WJmlQ92zufl7bffNl9ipUuXdq/TczE999xzZv/fddddMnHiRJ+/P25Hv5hjH3806MT+nOvnW+d708+6nkdSaxHVrl27pEmTJuaxdULcNm3ayD///CP+pK9xwYIFvY57+v8nnnhCihcvLuvXr/daru/Zzz77zJzxQGuW9DV4/vnn5fTp02YbfT/rNkqPzfr+19fHOV2UBhW9X31v6zF17ty5Xvev2//www/mVFN6jNbPwu3obbQc+jx034aFhZlQpPtfP+M3q6WcNGmSlCpVyhw3dX889dRTCT4ua8jVfayPqbfX44E+N6eJV+l3gpbNuZ6QY7VuP3nyZHNM0feyfhaSE4EoSKROnVrGjx8vu3fvNiFn5cqVXh8GdenSJfOF9t///tdsp9s/88wz7gOjXmrWrOnevn///tKrVy/zhaZfYnpAdt68+utJP3j6odD1ehAYPnx4osutX3T6Ja2/0LS2QZ+HfpD04BFbdHS0PP300+bx9MtZw5j+bdu2rXTt2lX27NkjH330kTn4J/cHKVD273vvvWcO2GvWrDGBUfefQwOrvjYff/yxOdDqCYvnz5+foMfXLzG9aDW47oc7oSedbNGihTz22GNmH77yyivSr18/r230oKvvR62N2rFjh8yZM8eU1ZYJVfUzoL/2ly9fLosWLXIvf/fdd80X6LZt28xrpu933caX7w9f0jJoWNIaCm2W0kBdt25dqVy5svms6xeu1jDq8cff9NiltT8O/b+Gh4ceesi9/PLly+aYp9tqrYU2Pf3666/m86AhyAk9GkbnzZtn/q/7UY+p2qSoNDB8+umn5vXQ/dS9e3d54YUX5KeffvIqj+7fUaNGyd69e6VChQp39Jzy588vrVu3Nj+A9YdQbLoP9IeGNq1pOXV/1KlTJ8HHZX2v6X1/9dVX5vazZs1yB59NmzaZv1pDpc/fuZ7QY7U2z+pj7dy50/z4TVY6MSMCX7t27Vxp0qRxZcmSxevy9ttvx7v9119/7cqTJ4/7+owZM7QqwLV9+/Y49/vEE094LTt06JDZ9r///a972e7du82yvXv3muvPPfecq2nTpl63a9WqlStHjhy3vO+uXbu6HnrooZs+z7///ts8zs6dO73K8vPPP7vq1avnql27tuvcuXPu7XXZiBEjvO7js88+cxUsWNCV0sT3et3MzfbvwYMH3csmTpzoKlCggPu6viajR492X7927Zrr7rvvTvBjzp0715UrVy5XxowZXTVr1nSFh4e7fv31V69ttAzz58/32nfbtm0z13X70NBQr+379u1rtjl79qy5HhYW5urQoYPXNrrvU6dO7bp8+bIrpdL3vL73Y9P95nxmdP/r/oqOjvbapmjRoq7GjRvH+aw1adLEp++PWxk0aJDZB57Hnvvuu89dbs/3kD7XypUre91+2LBhroYNG3otO3bsmCnTvn37XP40bdo083z08xAVFeVKmzat6/Tp064vvvjCVadOHbPNihUrTFmPHDkS5/abNm0y686fP2+ur1q1yus9ra5cueLKnDmza926dV631fe7Hks9b7dgwYIEl93z/RPb5MmTzf2dOnUqzntw3rx5ruzZs5vnmxB/xzoud+7c2VW3bl1XTExMvNt7HgcSc6zW23Xr1s3lL9QQpSD660R/WXteXnvtNbNO2/br1atnqtO1Klero7Vvh/4qdOgvtsT84vDcVqtGlVM1rL9etPOepxo1aiT6OR04cMDUPGn1qbaFO78ynOYwh26jv1qWLVvmdeZi/ZWmv3KcGgy9tG/f3vwy8XzuKV1C9q82XXr2b9B95uwv7WCpr4nnPtMmEK36TyittTl+/Lj5Zai1OFrFr9XfCe1Qm5D3jO5PvT/P/alNLvrL9NChQxLstNkrvv42sV8nva6vp6/eHwltXvI89jg1IfHRJp/Y+1VrWzz3qzZ7q9t1Uk5qWhukxxatydBaDK0Nz5cvn6khcvoR6Xtdj1FaK71lyxZTy6n/19dat4vvmOXp4MGDZl80aNDA6zXQGqPYzz8xn8lbcZrD4+srquXQZi59Tvpe0RqeSx7vldsdl7VGTN8D+p7QmiY9Lt9OQo/Vvnr+d4JO1SmItqmWLFkyznKtstU2Xh0BpNWP2nlOmxm0SUvbevVAqLTdOjEdqT07szm3i68p62a0mjV2HxWtbvakBxb9YE6bNs30m9D711FJWm5PTZs2lc8//9z0Y9Kqd4e2k2s7tDbFxOZ0hkzpErp/Y3c+1H3m6zPz6GuqB1O9DBgwwDR76fmTnCaD/5Xuz1dffdUcZGPTL6CUSr9UYo/6UdqU5Bnw9TMeqO8PDWrxHX/iE/t56H7Vz7o22cfm/NjyF31Od999twlsZ8+edQccPR5pE5j2x9F1etzR4KQBXS8aIjQ4aUjQ67GPWZ6c/lyLFy82ofVWA2Pu5D0QHw3M+r7TPkCxaZDbunWrCXoaZvSk54MHDzahUPsN3e64rD+E9AeK9nfSMK5Nn/Xr1/fqExXfa5CQY7Wvnv+dIBAFAf3Fom9Y7SOiIURp225CD3LxtTHfjnbe019Pnjw7ICo9WGhHSk/6q8I5MOsvWG1/1g+ddqJVN+tEqAd7/UBqZzs9qDgHLf1g6n0k9EBt2/516JeufvHoPnP6Cmh/ML1vfQ3vlHbQT+jwWn3PaO3Srd4zWhbtXxBs+1N/Scf3K1q/lLRG4nZiv056XV9PX70/kpruV61R0pqG242a8lftu4YDDUS9e/d2L9fPin7pb9y40RyDfvvtN3Pc0j4+GpaU9rPx5NTweR5XnYEsGp6cY1dS0pq/L774wnR2d94Tsel+0BCjF/1RkzNnTtP3TMuXkOOyhq1WrVqZi3bI1lpj7ZeogVyP8bG/V1LCsTrw3pm4Ke3MevLkyThvan2Dac3LhAkTTLJ3OjImhB6gdOSJvlH1l4Tnr9Vb0V/wtWrVMp00dUSG3od2zPOkv6i0M6hWC2sVv9bwaEDSjpXOKAx9zKlTp5ovaz1YxO5k66lz587mQ6a/hvUgpaMg9JeNXtfaA/1Q6odfq2b1ce6kk7e/aS1C7IkMdaTOne5fT9qZUQ/kOrJEmyt0ZFJC50rRLwHt0K6dHLUpVX9h6hfB6NGjzf5PCG3e1S9t/cLRmiX9Io/d3Na3b18z/Fk7Ues2+mtRA5J2IP7www8lpdIvUy2/fm70eemXowb7L7/8Ur777rvb3l73ub7W+gWnr8XXX39tbq/+l89/cunYsaP5gtVmGO3srV+a2ow0e/ZsM8hDp2jwdyDSMurr6BlY9P/6XtSaEd1Gj7caePS11vezHmdiz+2jNSta+6ad4rVmW2vm9fOiHdi1I7WGVz126Wdd95UGi3bt2t1x2bWWT78XnGH3Wos+YsQIcyzXz3t8tGw6ilgDnx6HdfRnTEyMCe4JOS7rsUPX6bFcj7n6ftSRbs4oY/1e0c7Y+h2h73W9zxRxrPZb7yUkinZc1N0V+1K6dGmzfuzYsaZzWqZMmVyNGjVyffrpp14d+27W+U47DzZo0MCVNWtWs7127IvdGVbp/TjrHdOnTzedcvUxH3vsMdd7770X5zEGDhxoOm7q8u7du7s6derk1al6+fLlrrJly7oyZMjgqlChgmv16tW37JirxowZ48qWLZtr7dq15vqSJUtMJ18th3YUvP/++11Tp051Bcs+1o6Xd7J/9TX0/Ihrp1HtVKmvUc6cOV09evRwtW3bNkGdqrVTaL9+/VxVqlQxj6MdRPW999Zbb7kuXbrk3u52++67775zlSxZ0uzvBx980PXxxx/H6YC6ceNG93tSO7vq++JmgwdSEud55cuXz7yG1atX9+p4erNO9dqpesiQIa6nn37avO4hISGuDz74wGsbX7w/btepumLFivGui69TdXwdyPfv3+968sknzXtPy1mmTBnTgfZmHXOTk/Ne1TJ5Onz4sNdxVmln62LFipn3cI0aNVwLFy6M8z4fOnSo2U+pUqUyr4/S5/n++++b+0qXLp15H+i++umnn27aGft2nM7yetHH0n2sxz99/MjISK9tPfeLDlTQ6zpIQveFfsbmzJmT4OOyHl8rVapkPp96PNEO01u3bnXfXl8T/ZxrB3V9/zpud6yOrzN2ckr1/woB/M/0177Oc8FpGAAAKQ2jzAAAgPUIRABMPwHP4bCxL7caUoyU71b7Xoeiw3+nS7nZftFRbvAtmswAmBFn8Z1GwxGoo4PgG9rB+WZ0mLh2DEbyO3LkSJypShx6ug3trA3fIRABAADr0WQGAACsRyACAADWIxABAADrEYgAWEFnD07oaUYA2IdABCAo6OkL9PQueoZuPV2AnmtKT2WhpxAAgNthHC2AFE+nDNDzJum5lPT8eeXLlzfDlfUce3qOKj0pJwDcCjVEAFK8N954wzSJ6VnJW7Zsac4gr5Pa9ejRI86Z4j1PJKvbZc6c2dQqDRgwwGvOFz3xpJ7QU+d60RNwVq1a1X1mc50fRmuf9KSVegJafSw9QSaAlIsaIgAp2pkzZ2TJkiXy9ttvm3ASm3MG7tg06Oj59woVKiQ7d+6U9u3bm2V6NnbVunVrczbvyZMnm7Oxb9++XdKlS2fWaa2TngF9zZo15jH37NljZg8GkHIRiACk+FmWdX7ZMmXKJOp2b731ltdM3L169ZLZs2e7A5GerqR3797u+y1VqpR7e12nNVHaNKe0hglAykaTGYAU7U4n258zZ47pdxQSEmJqdzQgeZ6zTZvbXnnlFalfv76MGjVKfv/9d/e6Ll26yPDhw83tBw0aJDt27PDJcwHgPwQiACma1txo/6HEdJyOiIgwTWJNmzaVRYsWybZt26R///6mGcwxePBg2b17tzRr1kxWrlwpoaGhMn/+fLNOg9Iff/whbdq0Mc1t1apVkwkTJiTJ8wOQPDiXGYAUr0mTJiaY7Nu3L04/onPnzpl+RBqaNNA0b95cxowZI5MmTfKq9dGQM3fuXLN9fJ577jm5ePGiLFy4MM668PBwWbx4MTVFQApGDRGAFG/ixIly48YNuf/++2XevHly4MAB2bt3r4wfP15q1KgRb62SNo9pnyENRbqdU/ujLl++LJ06dZLVq1ebEWVr166VTZs2SdmyZc36bt26mSH9hw4dkq1bt8qqVavc6wCkTHSqBpDiaadmDSY60qxnz55y4sQJyZcvnxkqr6PEYnv88cele/fuJvRER0ebZjEddq/NZEpHlf3777/Stm1bOXXqlOTNm1datGghQ4YMMes1fOlIsz///NMMyW/cuLGMGzcu2Z83AN+hyQwAAFiPJjMAAGA9AhEAALAegQgAAFiPQAQAAKxHIAIAANYjEAEAAOsRiAAAgPUIRAAAwHoEIgAAYD0CEQAAsB6BCAAAWI9ABAAAxHb/B0BErtB2eeOdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = dataset.class_names\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "for images, labels in dataset.unbatch():\n",
    "    class_idx = np.argmax(labels.numpy())\n",
    "    class_counts[class_names[class_idx]] += 1\n",
    "\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d3301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988c5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1942 files belonging to 4 classes.\n",
      "Using 1360 files for training.\n",
      "Found 1942 files belonging to 4 classes.\n",
      "Using 582 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    target_root,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    target_root,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "val_ds = val_ds.skip(val_batches // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7659b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2), \n",
    "    layers.RandomZoom(0.2),  \n",
    "    layers.RandomContrast(0.2),  \n",
    "    layers.RandomBrightness(0.2), \n",
    "    layers.RandomTranslation(0.1, 0.1), \n",
    "])\n",
    "preprocessing = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepare(ds, augment=False):\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda x, y: (preprocessing(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds_prep = prepare(train_ds, augment=True)\n",
    "val_ds_prep = prepare(val_ds)\n",
    "test_ds_prep = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987354ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pixel value: 1.0\n",
      "Min pixel value: 0.0\n",
      "Image batch shape: (32, 224, 224, 3)\n",
      "Label batch shape: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds_prep.take(1):\n",
    "    print(\"Max pixel value:\", images.numpy().max())\n",
    "    print(\"Min pixel value:\", images.numpy().min())\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7d4c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               8652900   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,754,824\n",
      "Trainable params: 8,754,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(224, 224, 3)),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0853f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 14s 78ms/step - loss: 1.0270 - accuracy: 0.5603 - val_loss: 0.9412 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 5s 88ms/step - loss: 0.9549 - accuracy: 0.6118 - val_loss: 0.9534 - val_accuracy: 0.6531 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 5s 90ms/step - loss: 0.9228 - accuracy: 0.6169 - val_loss: 0.9419 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 5s 94ms/step - loss: 0.9232 - accuracy: 0.6221 - val_loss: 0.8533 - val_accuracy: 0.6973 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 5s 92ms/step - loss: 0.8943 - accuracy: 0.6353 - val_loss: 0.8587 - val_accuracy: 0.6837 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 5s 92ms/step - loss: 0.8874 - accuracy: 0.6279 - val_loss: 0.8673 - val_accuracy: 0.6497 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 5s 91ms/step - loss: 0.8670 - accuracy: 0.6346 - val_loss: 0.8336 - val_accuracy: 0.6633 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 4s 88ms/step - loss: 0.8575 - accuracy: 0.6412 - val_loss: 0.7881 - val_accuracy: 0.6803 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 5s 92ms/step - loss: 0.8580 - accuracy: 0.6404 - val_loss: 0.8021 - val_accuracy: 0.6565 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "cnn_callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_cnn_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    train_ds_prep,\n",
    "    validation_data=val_ds_prep,\n",
    "    epochs=20,\n",
    "    callbacks = cnn_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2c5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model saved!\n"
     ]
    }
   ],
   "source": [
    "cnn_model.save('final_cnn_model.keras')\n",
    "print(\"CNN model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a44012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 33ms/step - loss: 0.9141 - accuracy: 0.6944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9141314029693604, 0.6944444179534912]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(test_ds_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61e5297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "y_true_1 = []\n",
    "y_pred_1 = []\n",
    "\n",
    "for images, labels in train_ds_prep:\n",
    "    preds = cnn_model.predict(images)\n",
    "    y_true_1.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_1.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true_2 = []\n",
    "y_pred_2 = []\n",
    "\n",
    "for images, labels in test_ds_prep:\n",
    "    preds = cnn_model.predict(images)\n",
    "    y_true_2.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_2.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d28aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Classification Report:\n",
      "Train Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.00      0.00      0.00        22\n",
      "    Land_Slide       0.00      0.00      0.00       338\n",
      "    Urban_Fire       0.83      0.59      0.69       290\n",
      "Water_Disaster       0.60      0.97      0.74       710\n",
      "\n",
      "      accuracy                           0.63      1360\n",
      "     macro avg       0.36      0.39      0.36      1360\n",
      "  weighted avg       0.49      0.63      0.53      1360\n",
      "\n",
      "Test Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.00      0.00      0.00         8\n",
      "    Land_Slide       0.00      0.00      0.00        59\n",
      "    Urban_Fire       0.83      0.53      0.65        66\n",
      "Water_Disaster       0.62      0.98      0.76       155\n",
      "\n",
      "      accuracy                           0.65       288\n",
      "     macro avg       0.36      0.38      0.35       288\n",
      "  weighted avg       0.52      0.65      0.56       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Classification Report:\")\n",
    "print(\"Train Set:\")\n",
    "print(classification_report(y_true_1, y_pred_1, target_names=class_names, zero_division=0))\n",
    "print(\"Test Set:\")\n",
    "print(classification_report(y_true_2, y_pred_2, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2bd9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29084464/29084464 [==============================] - 5s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " densenet121 (Functional)    (None, 7, 7, 1024)        7037504   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1024)             4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,337,412\n",
      "Trainable params: 297,860\n",
      "Non-trainable params: 7,039,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = DenseNet121(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "densenet_model = Model(inputs, outputs)\n",
    "\n",
    "densenet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "densenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7fc041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "43/43 [==============================] - 16s 206ms/step - loss: 0.9492 - accuracy: 0.6228 - val_loss: 0.4998 - val_accuracy: 0.8027 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 6s 125ms/step - loss: 0.6433 - accuracy: 0.7529 - val_loss: 0.4890 - val_accuracy: 0.7959 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 6s 127ms/step - loss: 0.5890 - accuracy: 0.7860 - val_loss: 0.4455 - val_accuracy: 0.8231 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.5280 - accuracy: 0.7978 - val_loss: 0.4361 - val_accuracy: 0.8673 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4948 - accuracy: 0.8110 - val_loss: 0.3999 - val_accuracy: 0.8741 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "43/43 [==============================] - 6s 115ms/step - loss: 0.4558 - accuracy: 0.8176 - val_loss: 0.3900 - val_accuracy: 0.8299 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "43/43 [==============================] - 6s 122ms/step - loss: 0.4522 - accuracy: 0.8279 - val_loss: 0.4494 - val_accuracy: 0.8469 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "43/43 [==============================] - 6s 120ms/step - loss: 0.4333 - accuracy: 0.8316 - val_loss: 0.4319 - val_accuracy: 0.8401 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "43/43 [==============================] - 6s 122ms/step - loss: 0.4444 - accuracy: 0.8199 - val_loss: 0.4098 - val_accuracy: 0.8673 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4159 - accuracy: 0.8441 - val_loss: 0.3067 - val_accuracy: 0.9116 - lr: 5.0000e-04\n",
      "Epoch 11/15\n",
      "43/43 [==============================] - 6s 123ms/step - loss: 0.3651 - accuracy: 0.8522 - val_loss: 0.4052 - val_accuracy: 0.8673 - lr: 5.0000e-04\n",
      "Epoch 12/15\n",
      "43/43 [==============================] - 6s 125ms/step - loss: 0.3676 - accuracy: 0.8588 - val_loss: 0.3721 - val_accuracy: 0.8707 - lr: 5.0000e-04\n",
      "Epoch 13/15\n",
      "43/43 [==============================] - 6s 121ms/step - loss: 0.3820 - accuracy: 0.8537 - val_loss: 0.4276 - val_accuracy: 0.8707 - lr: 5.0000e-04\n",
      "Epoch 14/15\n",
      "43/43 [==============================] - 6s 124ms/step - loss: 0.3675 - accuracy: 0.8493 - val_loss: 0.3063 - val_accuracy: 0.8878 - lr: 2.5000e-04\n",
      "Epoch 15/15\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.3373 - accuracy: 0.8684 - val_loss: 0.3231 - val_accuracy: 0.8776 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "densenet_callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_densenet_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "history_densenet = densenet_model.fit(\n",
    "    train_ds_prep,\n",
    "    validation_data=val_ds_prep,\n",
    "    epochs=15,\n",
    "    callbacks = densenet_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae4798a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model.save('densenet_before_finetuned.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34caefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 74ms/step - loss: 0.4087 - accuracy: 0.8715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4086802303791046, 0.8715277910232544]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densenet_model.evaluate(test_ds_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1fc0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "y_true_1 = []\n",
    "y_pred_1 = []\n",
    "\n",
    "for images, labels in train_ds_prep:\n",
    "    preds = densenet_model.predict(images)\n",
    "    y_true_1.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_1.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true_2 = []\n",
    "y_pred_2 = []\n",
    "\n",
    "for images, labels in test_ds_prep:\n",
    "    preds = densenet_model.predict(images)\n",
    "    y_true_2.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_2.extend(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eff3c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121 Before Fine Tuning Classification Report:\n",
      "Train Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.43      0.27      0.33        22\n",
      "    Land_Slide       0.81      0.82      0.82       338\n",
      "    Urban_Fire       0.99      0.93      0.96       290\n",
      "Water_Disaster       0.91      0.94      0.92       710\n",
      "\n",
      "      accuracy                           0.90      1360\n",
      "     macro avg       0.78      0.74      0.76      1360\n",
      "  weighted avg       0.89      0.90      0.89      1360\n",
      "\n",
      "Test Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.33      0.14      0.20         7\n",
      "    Land_Slide       0.71      0.78      0.74        58\n",
      "    Urban_Fire       0.94      0.79      0.86        61\n",
      "Water_Disaster       0.89      0.94      0.92       162\n",
      "\n",
      "      accuracy                           0.86       288\n",
      "     macro avg       0.72      0.66      0.68       288\n",
      "  weighted avg       0.85      0.86      0.85       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DenseNet121 Before Fine Tuning Classification Report:\")\n",
    "print(\"Train Set:\")\n",
    "print(classification_report(y_true_1, y_pred_1, target_names=class_names, zero_division=0))\n",
    "print(\"Test Set:\")\n",
    "print(classification_report(y_true_2, y_pred_2, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5880002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " densenet121 (Functional)    (None, 7, 7, 1024)        7037504   \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1024)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,337,412\n",
      "Trainable params: 7,251,716\n",
      "Non-trainable params: 85,696\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model_2 = DenseNet121(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model_2.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(224, 224, 3))\n",
    "x = base_model_2(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "densenet_model_2 = Model(inputs, outputs)\n",
    "\n",
    "densenet_model_2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "densenet_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c5a6f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "43/43 [==============================] - 27s 376ms/step - loss: 1.3257 - accuracy: 0.4574 - val_loss: 56197340.0000 - val_accuracy: 0.5578 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "43/43 [==============================] - 13s 284ms/step - loss: 1.2079 - accuracy: 0.5176 - val_loss: 460.7950 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "43/43 [==============================] - 13s 283ms/step - loss: 1.0163 - accuracy: 0.5853 - val_loss: 808983.6875 - val_accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "43/43 [==============================] - 14s 303ms/step - loss: 1.0787 - accuracy: 0.5721 - val_loss: 2.3049 - val_accuracy: 0.6088 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "43/43 [==============================] - 13s 286ms/step - loss: 0.9870 - accuracy: 0.6029 - val_loss: 1.1321 - val_accuracy: 0.5646 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "43/43 [==============================] - 13s 285ms/step - loss: 1.0312 - accuracy: 0.5816 - val_loss: 2.6602 - val_accuracy: 0.2517 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "43/43 [==============================] - 13s 295ms/step - loss: 1.0122 - accuracy: 0.5912 - val_loss: 1.3874 - val_accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "43/43 [==============================] - 14s 309ms/step - loss: 0.9945 - accuracy: 0.5971 - val_loss: 1.5073 - val_accuracy: 0.5306 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "43/43 [==============================] - 15s 316ms/step - loss: 0.9516 - accuracy: 0.6074 - val_loss: 1.4530 - val_accuracy: 0.5510 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "43/43 [==============================] - 15s 319ms/step - loss: 0.9162 - accuracy: 0.5949 - val_loss: 1.2398 - val_accuracy: 0.5476 - lr: 5.0000e-04\n",
      "Epoch 11/15\n",
      "43/43 [==============================] - 15s 320ms/step - loss: 0.9138 - accuracy: 0.6081 - val_loss: 1.2149 - val_accuracy: 0.5408 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "densenet_callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7),\n",
    "    ModelCheckpoint('best_densenet_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "history_densenet_2 = densenet_model_2.fit(\n",
    "    train_ds_prep,\n",
    "    validation_data=val_ds_prep,\n",
    "    epochs=15,\n",
    "    callbacks = densenet_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26c57c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model.save('final_densenet_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1acfdae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 76ms/step - loss: 0.4602 - accuracy: 0.8646\n"
     ]
    }
   ],
   "source": [
    "densenet_model.evaluate(test_ds_prep)\n",
    "\n",
    "y_true_1 = []\n",
    "y_pred_1 = []\n",
    "\n",
    "for images, labels in train_ds_prep:\n",
    "    preds = densenet_model.predict(images, verbose=0)\n",
    "    y_true_1.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_1.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true_2 = []\n",
    "y_pred_2 = []\n",
    "\n",
    "for images, labels in test_ds_prep:\n",
    "    preds = densenet_model.predict(images, verbose=0)\n",
    "    y_true_2.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred_2.extend(np.argmax(preds, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eca7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121 Classification Report:\n",
      "Train Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.50      0.32      0.39        22\n",
      "    Land_Slide       0.84      0.83      0.83       338\n",
      "    Urban_Fire       0.97      0.93      0.95       290\n",
      "Water_Disaster       0.92      0.95      0.93       710\n",
      "\n",
      "      accuracy                           0.90      1360\n",
      "     macro avg       0.81      0.76      0.78      1360\n",
      "  weighted avg       0.90      0.90      0.90      1360\n",
      "\n",
      "Test Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    Earthquake       0.00      0.00      0.00         4\n",
      "    Land_Slide       0.75      0.81      0.78        57\n",
      "    Urban_Fire       0.94      0.84      0.89        58\n",
      "Water_Disaster       0.91      0.94      0.93       169\n",
      "\n",
      "      accuracy                           0.88       288\n",
      "     macro avg       0.65      0.65      0.65       288\n",
      "  weighted avg       0.88      0.88      0.88       288\n",
      "\n",
      "Class names saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"DenseNet121 Classification Report:\")\n",
    "print(\"Train Set:\")\n",
    "print(classification_report(y_true_1, y_pred_1, target_names=class_names, zero_division=0))\n",
    "print(\"Test Set:\")\n",
    "print(classification_report(y_true_2, y_pred_2, target_names=class_names, zero_division=0))\n",
    "\n",
    "import json\n",
    "with open('class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f)\n",
    "print(\"Class names saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b7a7876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 120). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PC\\AppData\\Local\\Temp\\tmp6qdxmjpp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\PC\\AppData\\Local\\Temp\\tmp6qdxmjpp\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('final_densenet_model.keras')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"densenet.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
